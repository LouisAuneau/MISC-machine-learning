{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Basic 2 layers NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic 2 layers neural network using sigmoid as activation function, and no bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05206804]\n",
      " [0.97087554]\n",
      " [0.97141422]\n",
      " [0.99192991]]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Neural Network class modeling a 2 layer NN without biases.\n",
    "\n",
    "    Attributes:\n",
    "        input                 (np.ndarray)               : Input training data.\n",
    "        y                     (np.ndarray)               : Training data real output.\n",
    "        n_features            (int)                      : Number of features (characteristics) in the training data.\n",
    "        weights               (List[np.ndarray])         : Weights W with which are multiplied features before applying activation function.\n",
    "        outputs               (List[np.ndarray])         : Output of each layer (Index 0 is layer 1, 1 is the output layer). Is updated at each training iteration.\n",
    "        activation            (Callable[[float], float]) : Activation function of the neurons.\n",
    "        activation_derivative (Callable[[float], float]) : Derivative of the activation function of the neurons.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, activation: Callable[[float], float], activation_derivative: Callable[[float], float]) -> NeuralNetwork:\n",
    "        \"\"\"\n",
    "        Creates a neural network with the training data in input.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Matrix NxM where N is the number of training examples and M the number of features.\n",
    "            y (np.array)  : Array of training results of size N\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "        self.y = y\n",
    "        self.n_features = x.shape[1]\n",
    "        self.weights = [np.nan, np.nan] # Layer weights\n",
    "        self.weights[0] = np.random.rand(self.n_features, 4)\n",
    "        self.weights[1] = np.random.rand(4, 1)\n",
    "        self.outputs = [np.zeros((4, 1)), np.zeros(y.shape)] # Layers outputs\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        \n",
    "    def feed_forward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Feed forward algorithm, dot product of layer input with its weights, and then applying activation function to it. Gives layers outputs.\n",
    "        \"\"\"\n",
    "        self.outputs[0] = self.activation(np.dot(self.input, self.weights[0]))\n",
    "        self.outputs[1] = self.activation(np.dot(self.outputs[0], self.weights[1]))\n",
    "    \n",
    "    def backward_propagation(self) -> None:\n",
    "        \"\"\"\n",
    "        Backward propagation algorithm, using dervative of the cost function, with chain rule to include weight in it, to update weights.\n",
    "        Calculus detail : https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
    "        \"\"\"\n",
    "        self.weights[1] += np.dot(self.outputs[0].T, (2 * (self.y - self.outputs[1])) * self.activation_derivative(self.outputs[1]))\n",
    "        self.weights[0] += np.dot(self.input.T, (np.dot(2 * (self.y - self.outputs[1]) * self.activation_derivative(self.outputs[1]), self.weights[1].T) *  self.activation_derivative(self.outputs[0])))\n",
    "\n",
    "\n",
    "def sigmoid(x: float, _lambda: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    Sigmoid function. https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "    \n",
    "    Args:\n",
    "        x       (float) : \n",
    "        _lambda (float) : (Prefixed with underscore because lambda is a reserved keyword of Python)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(- _lambda * x))\n",
    "\n",
    "def sigmoid_derivative(x: float, _lambda: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    Derivative of the sigmoid function.\n",
    "    \n",
    "    Args:\n",
    "        x       (float) : \n",
    "        _lambda (float) : (Prefixed with underscore because lambda is a reserved keyword of Python)\n",
    "    \"\"\"\n",
    "    return _lambda * x * (1 - x)\n",
    "    \n",
    "\n",
    "# Main\n",
    "input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # AND truth table\n",
    "output = np.array([[0], [1], [1], [1]]) # AND truth table\n",
    "n_iterations = 500\n",
    "nn = NeuralNetwork(input, output, sigmoid, sigmoid_derivative)\n",
    "\n",
    "# --> Training\n",
    "for i in range(0, n_iterations):\n",
    "    nn.feed_forward()\n",
    "    nn.backward_propagation()\n",
    "\n",
    "# --> Prediction\n",
    "print(nn.outputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
